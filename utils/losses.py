# -*- coding: utf-8 -*-
"""losses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1diMxj63bibGSyJFXpjMkPSHNy_ObKgaX
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Multi-label Focal Loss with optional positive class weighting.

    Args:
        alpha (float): Weighting factor for the class balance.
        gamma (float): Focusing parameter.
        reduction (str): Specifies the reduction to apply to the output.
        pos_weight (Tensor, optional): A weight of positive examples.
    """

    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean', pos_weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.pos_weight = pos_weight

    def forward(self, logits, targets):
        """
        Compute the focal loss between logits and targets.

        Args:
            logits (Tensor): Model raw outputs.
            targets (Tensor): Ground truth binary labels.

        Returns:
            Tensor: Computed loss.
        """
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets, reduction='none', pos_weight=self.pos_weight
        )

        probas = torch.sigmoid(logits)
        pt = torch.where(targets == 1, probas, 1 - probas)
        focal_term = (1 - pt) ** self.gamma
        loss = self.alpha * focal_term * bce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss