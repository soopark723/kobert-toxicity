# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NO_fmlpj8dImDJNxEcmUpFNa7KkzmYI1
"""

import os
import random
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer

def set_seed(seed: int = 42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

class HateSpeechDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def create_datasets(dataframe, tokenizer, test_size=0.2, val_size=0.1, random_state=42, max_length=128):
    train_val_df, test_df = train_test_split(dataframe, test_size=test_size, stratify=dataframe['label'], random_state=random_state)
    train_df, val_df = train_test_split(train_val_df, test_size=val_size, stratify=train_val_df['label'], random_state=random_state)

    train_dataset = HateSpeechDataset(train_df['text'].tolist(), train_df['label'].tolist(), tokenizer, max_length)
    val_dataset = HateSpeechDataset(val_df['text'].tolist(), val_df['label'].tolist(), tokenizer, max_length)
    test_dataset = HateSpeechDataset(test_df['text'].tolist(), test_df['label'].tolist(), tokenizer, max_length)

    return train_dataset, val_dataset, test_dataset